---
title: "Untitled"
author: "WHSP"
date: "28 de junho de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Regression analysis in R
Grant R. McDermott
University of Oregon | EC 607
Lecture 8
Today’s lecture is about the bread-and-butter tool of applied econometrics and data science: regression analysis. My goal is to give you a whirlwind tour of the key functions and packages. I’m going to assume that you already know all of the necessary theoretical background on causal inference, asymptotics, etc. This lecture will not cover any of theoretical concepts or seek to justify a particular statistical model. Indeed, most of the models that we’re going to run today are pretty silly. We also won’t be able to cover some important topics. For example, I’ll only provide the briefest example of a Bayesian regression model and I won’t touch times series analysis at all. (Although, I will provide links for further reading at the bottom of this document.) These disclaimers aside, let’s proceeed…

Software requirements
R packages
It’s important to note that “base” R already provides all of the tools we need for basic regression analysis. However, we’ll be using several external packages today, because they will make our lives easier and offer increased power for some more sophisticated analyses.

New: broom, estimatr, sandwich, lmtest, AER, lfe, plm, huxtable, margins
Already used: tidyverse, hrbrthemes, listviewer
The broom package was bundled with the rest of tidyverse and sandwich should get installed as a dependency of several of the above packages. Still, a convenient way to install (if necessary) and load everything is by running the below code chunk. I’ll also go ahead and set my preferred ggplot2 theme for the rest of this document.

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, broom, hrbrthemes, plm, estimatr, sandwich, lmtest, AER, lfe, huxtable, margins)
theme_set(hrbrthemes::theme_ipsum())
```

While we’ve already loaded all of the required packages for today, I’ll try to be as explicit about where a particular function is coming from, whenever I use it below.

Something else that I want to mention up front is that we’ll mostly be working with the starwars data frame that we’ve already seen from previous lectures. Here’s a quick reminder of what it looks like to refresh your memory.

```{r}
head(starwars)
```

---

Regression basics
The lm() function
R’s workhorse command for running regression models is the built-in lm() function. The “lm” stands for “linear models” and the syntax is very intuitive.1

`lm(y ~ x1 + x2 + x3 + ..., data = df)`

You’ll note that the lm() call includes a reference to the data source (in this case, a hyopthetical data frame called df). We covered this in our earlier lecture on R language basics and object-orientated programming, but the reason is that many objects (e.g. data frames) can exist in your R environment at the same time. So we need to be specific about where our regression variables are coming from — even if df is the only data frame in our global environment at the time. Another option would be to use indexing, but I find it a bit verbose:

`lm(df$y ~ df$x1 + df$x2 + df$x3 + ...)`

Let’s run a simple bivariate regression of starwars characters’ mass on height.

```{r}
ols1 <- lm(mass ~ height, data = starwars)
# ols1 <- lm(starwars$mass ~ starwars$height) ## Also works
ols1
```

---

The resulting object is pretty terse, but that’s only because it buries most of its valuable information — of which there is a lot — within its internal list structure. You can use the str() function to view this structure. Or, if you want to be fancy, the interactive listviewer::jsonedit() function that we saw in the previous lecture is nice option.

# str(ols1) ## Static option
```{r message=FALSE, warning=FALSE}
# str(ols1) ## Static option
listviewer::jsonedit(ols1, mode="view") ## Interactive option
```

---

As we can see, this ols1 object has a bunch of important slots… containing everything from the regression coefficients, to vectors of the residuals and fitted (i.e. predicted) values, to the rank of the design matrix, to the input data, etc. etc. To summarise the key pieces of information, we can use the — wait for it — generic summary() function. This will look pretty similar to the default regression output from Stata that many of you will be used to.

```{r}
summary(ols1)
```

---

We can then dig down further by extracting a summary of the regression coefficients:

```{r}
summary(ols1)$coefficients
```

---

Get “tidy” regression coefficients with the broom package
While it’s easy to extract regression coefficients via the summary() function, in practice I always use the broom package to do so. This package has a bunch of neat features to convert regression (and other statistical) objects into “tidy” data frames. This is especially useful because regression output is so often used as an input to something else, e.g. a plot of coefficients or marginal effects. Here, I’ll use broom::tidy(..., conf.int=T) to coerce the ols1 regression object into a tidy data frame of coefficient values and key statistics.

---

```{r}
library(broom)

tidy(ols1, conf.int = T)
```

---

Again, I could now pipe this tidied coeffients data frame to a ggplot2 call, using saying geom_pointrange() to plot the error bars. Feel free to practice doing this yourself now, but we’ll get to some explicit examples further below.

A related and also useful function is broom::glance(), which summarises the model “meta” data (R2, AIC, etc.) in a data frame.

```{r}
glance(ols1)
```

---

Regressing on subsetted or different data
Our simple model isn’t particularly good; our R2 is only 0.018. Different species and homeworlds aside, we may have an extreme outlier in our midst…

```{r warning=FALSE}
starwars %>%
  ggplot(aes(x=height, y=mass)) +
  geom_point(alpha=0.5) +
  geom_point(
    data = starwars %>% filter(mass==max(mass, na.rm=T)), 
    col="red"
    ) +
  geom_text(
    aes(label=name),
    data = starwars %>% filter(mass==max(mass, na.rm=T)), 
    col="red", vjust = 0, nudge_y = 25
    ) +
  labs(
    title = "Spot the outlier...",
    caption = "Aside: Always plot your data!"
    )
```

---

Maybe we should exclude Jabba from our regression? You can do this in two ways: 1) Create a new data frame and then regress, or 2) Subset the original data frame directly in the lm() call.

1) Create a new data frame
Recall that we can keep multiple objects in memory in R. So we can easily create a new data frame that excludes Jabba using  dplyr::filter().

```{r}
starwars2 <-
  starwars %>% 
  filter(name != "Jabba Desilijic Tiure")
  # filter(!(grepl("Jabba", name))) ## Regular expressions also work

ols2 <- lm(mass ~ height, data = starwars2)
summary(ols2)
```

---

2) Subset directly in the lm() call
Running a regression directly on a subsetted data frame is equally easy.

```{r}
ols2a <- lm(mass ~ height, data = starwars %>% filter(!(grepl("Jabba", name))))
summary(ols2a)
```

---

The overall model fit is much improved by the exclusion of this outlier, with R2 increasing to 0.58. Still, we should be cautious about throwing out data. Another approach is to handle or account for outliers with statistical methods. Which provides a nice segue to robust and clustered standard errors.

Robust and clustered standard errors
Dealing with statistical irregularities (heteroskedasticity, clustering, etc.) is a fact of life for empirical researchers. However, it says something about the economics profession that a random stranger could walk uninvited into a live seminar and ask, “How did you cluster your standard errors?”, and it would likely draw approving nods from audience members.

The good news is that there are lots of ways to get robust and clustered standard errors in R. For many years, these have been based on the excellent sandwich package. However, my prefered way these days is to use the estimatr package, which is both fast and provides convenient aliases for the standard regression functions. For example, you can obtain robust standard errors using estimatr::lm_robust(). Let’s illustrate by running a robust version of the ols1 regression that ran earlier.

```{r}
# library(estimatr) ## Already loaded
ols1_robust <- lm_robust(mass ~ height, data = starwars)
tidy(ols1_robust, conf.int = T)
```

---

The package defaults to using Eicker-Huber-White robust standard errors, commonly referred to as “HC2” standard errors. You can easily specify alternate methods using the se_type = argument.2 For example, you can specify Stata robust standard errors if you want to replicate code or results from that language. (See here for more details on why this isn’t the default and why Stata’s robust standard errors differ from those in R and Python.)

```{r}
ols1_robust_stata <- lm_robust(mass ~ height, data = starwars, se_type = "stata")
tidy(ols1_robust_stata, conf.int = T)
```

---

The estimatr package also supports (robust) instrumental variable regression and clustered standard errors. I’ll return to these issues in the relevant sections below, but here’s a quick example of the latter just to illustrate:

```{r}
ols1_robust_clustered <- lm_robust(mass ~ height, data = starwars, clusters = homeworld)
```

```{r}
tidy(ols1_robust_clustered, conf.int = T)
```

---

Aside on HAC (Newey-West) standard errors
On thing I want to flag is that the estimatr package does not yet offer support for HAC (i.e. heteroskedasticity and autocorrelation consistent) standard errors a la Newey-West. I’ve submitted a feature request on GitHub — vote up if you would like to see it added sooner! — but you can still obtain these pretty easily using the aforementioned sandwich package. For example, we can use sandwich::NeweyWest() on our existing ols1 object to obtain HAC SEs for it.

```{r}
# library(sandwich) ## Already loaded
NeweyWest(ols1) ## Print the HAC VCOV

sqrt(diag(NeweyWest(ols1))) ## Print the HAC SEs
```

---

If you wanted to convert it to a tidy data frame of coefficient values, then, I would recommend first piping it to  lmtest::coeftest(..., vcov=NeweyWest), which is a convenient way to do hypothesis testing using alternate variance-covariance matrices. Note that in the below, I’m going to manually create my own upper and lower 95% confidence intervals, since broom::tidy(conf.int=T) doesn’t work with coeftest objects.

```{r}
# library(lmtest) ## Already loaded
ols1 %>% 
  lmtest::coeftest(vcov=NeweyWest) %>%
  tidy() %>% ## "conf.int" doesn't work with coeftest object, so calculate manually...
  mutate(
    conf.low = estimate - qt(0.975, df=ols1$df.residual)*std.error,
    conf.high = estimate + qt(0.975, df=ols1$df.residual)*std.error
    )
```

---

Dummy variables and interaction terms
Dummy variables as factors
Dummy variables are a core component of many regression models. However, these can be a pain to create in many statistical languages, since you first have to tabulate a whole new matrix of binary variables and then append it to the original data frame. In contrast, R has a much more convenient framework for creating and evaluating dummy variables in a regression. You simply specify the variable of interest as a factor.3

For this next section, it will be convenient to demonstrate using a subsample of the data that comprises only humans. I’ll first create this humans data frame and then demonstrate the dummy-variables-as-factors approach.

```{r}
humans <- 
  starwars %>% 
  filter(species=="Human") %>%
  mutate(gender_factored = as.factor(gender)) %>% ## create factored version of "gender"
  select(contains("gender"), everything())
humans
```

```{r}
ols_dv <- lm(mass ~ height + gender_factored, data = humans)
summary(ols_dv)
```

---

In fact, I’m even making things more complicated than they need to be. R is “friendly” and tries to help whenever it thinks you have misspecified a function or variable. While this is something to be aware of, it normally just worksTM. A case in point is that we don’t actually need to specify a qualitative or character variable as a factor in a regression. R will automatically do this for you regardless, since that’s the only sensible way to include string variables in a regression.

```{r}
## Use the non-factored "gender" variable instead
ols_dv2 <- lm(mass ~ height + gender, data = humans)
summary(ols_dv2)
```

Interaction effects
Like dummy variables, R provides a convenient syntax for specifying interaction terms directly in the regression model without having to create them manually beforehand.4 You can just use x1:x2 (to include only the interaction term) or  x1*x2 (to include the parent terms and interaction terms). Generally speaking, you are best advised to include the parent terms alongside an interaction term. This makes the * option a good default.

For example, we might wonder whether the relationship between a person’s body mass and their height is modulated by their gender. That is, we want to run a regression of the form

$$Mass=β0+β1DMale+β2Height+β3DMale×Height$$

To implement this in R, we simply run the following

```{r}
ols_ie <- lm(mass ~ gender*height, data = humans)
summary(ols_ie)
```

---

Panel models
Fixed effects with the lfe package
The simplest (and least efficient) way to include fixed effects in a regression model is, of course, to use dummy variables. However, it isn’t very efficient or scaleable. What’s the point learning all that stuff about the Frisch-Waugh-Lovell theorem, within-group transformations, etcetera, etcetera if we can’t use them in our software routines? Again, there are several options to choose from here. For example, the venerable plm package, which also handles random effects and pooling models. However, I am going to strongly advocate for the lfe package.

lfe (i.e. “linear fixed effects”) is one of my packages in the entire R catalogue. It has a boatload of functionality built in to it (instrumental variables support, multilevel clustering, etc.) It is also fast because it automatically uses all the available processing power on your machine. We’ll return to the idea of multicore implementation when we get to the lecture on parallel processing. For the moment, simply enjoy the fact that lfe is optimised to solve big regression problems as quickly as possible.

Let’s take a look, starting off with a simple example and then moving on to something more demanding.

Simple FE model
The package’s main function is lfe::felm(), which is used for estimating fixed effects linear models. The syntax is such that you first specify the regression model as per normal, and then list the fixed effect(s) after a |. An example may help to illustrate. Let’s say that we again want to run our simple regression of mass on height, but this time control for species-level fixed effects.

```{r}
library(lfe)

ols_fe <- felm(mass ~ height | species, data = starwars) ## Fixed effect(s) go after the "|"
coefs_fe <- tidy(ols_fe, conf.int = T)
summary(ols_fe)
```

---

ote that the resulting felm object drops all of the species intercepts, since it has abstracted them away as fixed effects.

High dimensional FEs and (multiway) clustering
One reason that I prefer the lfe package to other options — e.g. the panel-focused plm package (see further below) — is because it supports high dimensional fixed effects and (multiway) clustering.5 In the below example, I’m going to add “homeworld” as an additional fixed effect to the model and also cluster according to this variable. I’m not claiming that this is a particularly good or sensible model, but just go with it. Note that, since we specify “homeworld” in the fixed effects slot below, felm() automatically converts it to a factor even though we didn’t explicitly tell it to.

```{r}
ols_hdfe <- 
  felm(
    mass ~ height |
      species + homeworld | ## Two fixed effects go here after the first "|"
      0 | ## This is where your IV equation goes, but we put 0 since we aren't instrumenting.
      homeworld, ## The final slot is where we specify our cluster variables
    data = starwars)
coefs_hdfe <- tidy(ols_hdfe, conf.int = T)
coefs_hdfe
```

---

Visually, we can easily compare changes in the coefficients across models thanks to the fact that we saved the output in data frames with broom::tidy() above.

```{r}
bind_rows(
  coefs_fe %>% mutate(reg = "Model 4 (FE and no clustering)"),
  coefs_hdfe %>% mutate(reg = "Model 5 (HDFE and clustering)")
  ) %>%
  ggplot(aes(x=reg, y=estimate, ymin=conf.low, ymax=conf.high)) +
  geom_pointrange() +
  labs(Title = "Marginal effect of height on mass") +
  geom_hline(yintercept = 0, col = "orange") +
  ylim(-0.5, NA) +
  labs(
    title = "'Effect' of height on mass",
    caption = "Data: Characters from the Star Wars universe"
    ) +
  theme(axis.title.x = element_blank())
```

